# 训练准备说明文档

**创建时间**: 2026-01-04  
**训练目标**: 对所有地图类型进行PPO模型训练（88小时训练时间限制）

---

## 一、代码修改说明

### 1.1 修改的文件：`ccbsenv.py`

**修改位置**: `RewardCallback` 类（第266-316行）

**修改内容**:
- 添加收敛检测功能
- 添加训练进度监控和日志输出
- 添加最佳奖励跟踪

**关键修改点**:
```python
# 新增参数（都有默认值，向后兼容）:
- convergence_window=100        # 收敛检测窗口大小
- convergence_threshold=0.01    # 收敛阈值（奖励变化率）
- best_reward                   # 跟踪最佳奖励
- converged                     # 收敛状态标志

# 新增功能:
1. 收敛检测：当最近N个episode的奖励标准差小于阈值时，标记为收敛
2. 定期日志：每10个episode打印训练进度和收敛状态
3. 最佳奖励跟踪：记录训练过程中的最佳奖励值
```

**向后兼容性**: ✅ 完全兼容，原有代码无需修改

**核心逻辑不变**: 
- 返回值逻辑不变（达到max_episodes返回False，否则返回True）
- 训练停止条件不变
- PPO训练流程不变

---

### 1.2 新建的文件：`train_all_maps.py`

**文件说明**: 多地图类型训练脚本，支持批量训练所有地图类型

**主要功能**:
1. 支持训练多个地图类型（empty-16-16-random, room-64-64-8_random, den520d_random, warehouse-10-20-random）
2. 时间限制控制（88小时）
3. 检查点保存（每50个episode）
4. 详细日志记录（包含收敛状态）
5. 自动保存训练结果和奖励记录

**不影响原有代码**: ✅ 这是一个全新的独立脚本

---

## 二、超参数配置

### 2.1 PPO训练配置

```python
PPO_CONFIG = {
    "learning_rate": 2e-4,              # 学习率
    "gamma": 0.98,                      # 折扣因子
    "gae_lambda": 0.95,                 # GAE lambda参数
    "clip_range": 0.2,                  # PPO裁剪范围
    "ent_coef": 0.01,                   # 熵系数（鼓励探索）
    "total_timesteps_per_env": 10000,   # 每个环境的训练步数
    "max_episodes": 1000,               # 最大训练回合数
    "convergence_window": 100,          # 收敛检测窗口大小
    "convergence_threshold": 0.01,      # 收敛阈值（奖励变化率）
    "checkpoint_interval": 50,          # 检查点保存间隔（episode数）
}
```

### 2.2 CCBS算法配置

```python
CCBS_CONFIG = {
    "agent_size": 0.5,                      # 智能体尺寸 (0, 0.5]
    "hlh_type": 2,                          # 0-无hlh, 1-使用simplex求解lpp, 2-贪婪选择不相交冲突
    "precision": 0.1,                       # 等待时间确定精度
    "timelimit": 300,                       # 时间限制（秒）
    "use_precalculated_heuristic": False,   # True: 使用反向Dijkstra, False: 使用欧几里得距离
    "use_disjoint_splitting": True,         # 使用不相交分割
    "use_cardinal": True,                   # 优先处理cardinal冲突
    "use_corridor_symmetry": False,         # 使用走廊对称性
    "use_target_symmetry": False,           # 使用目标对称性
    "use_rl": False,                        # 训练时不需要RL模型
    "verbose": True,                        # 是否显示详细信息
}
```

### 2.3 强化学习环境配置

```python
RL_ENV_CONFIG = {
    "max_step": 1024,                  # 最大搜索步数
    "reward_1": 10,                    # 当前节点满足约束且无其他冲突的奖励
    "reward_2": 2,                     # 分支数量权重系数
    "reward_3": -5,                    # 当前节点不满足约束的惩罚
    "reward_iter_pos": -0.5,           # 迭代位置奖励
    "cost_weight": 0.01,               # 目标函数权重
    "cardinal_conflicts_weight": 1,    # cardinal冲突权重
    "semicard_conflicts_weight": 1,    # semi-cardinal冲突权重
    "non_cardinal_conflict_weight": 1, # non-cardinal冲突权重
    "max_process_agent": 100,          # 最大处理的智能体数
}
```

### 2.4 训练时间限制

```python
TRAINING_TIME_LIMIT_HOURS = 88  # 88小时训练时间限制
```

---

## 三、训练地图列表

本次训练将训练以下地图类型：

| 地图名称 | 地图路径 | 训练任务数 | 模型保存路径 |
|---------|---------|-----------|-------------|
| empty-16-16-random | instances/empty-16-16-random/map.xml | 24 | ppo_empty-16-16-random |
| room-64-64-8_random | instances/room-64-64-8_random/map.xml | 30 | ppo_room-64-64-8_random |
| den520d_random | instances/den520d_random/map.xml | 25 | ppo_den520d_random |
| warehouse-10-20-random | instances/warehouse-10-20-random/map.xml | 25 | ppo_warehouse-10-20-random |

**注意**: 
- roadmaps/sparse 已有训练好的模型 (ppo_road-sparse)，本次不重新训练
- empty-16-16-random 已有模型，但本次会重新训练

---

## 四、输出文件说明

### 4.1 模型文件
每个地图训练完成后会生成：
- `ppo_<地图名>.zip` - 训练好的模型文件（stable-baselines3格式）
- `ppo_<地图名>_checkpoint_ep<N>.zip` - 训练过程中的检查点文件（每50个episode保存一次）

### 4.2 日志文件
- `training_log_YYYYMMDD_HHMMSS.txt` - 完整的训练日志（包含所有输出和收敛状态）
- `rewards_<地图名>.csv` - 每个地图的奖励记录（包含episode和reward列）

### 4.3 日志内容
训练日志包含以下信息：
- 训练开始时间和时间限制
- 每个地图的训练进度
- 每个环境（训练任务）的训练状态
- 收敛检测结果（收敛状态、平均奖励、最佳奖励）
- 检查点保存记录
- 训练完成后的统计信息（训练时间、总回合数、收敛状态、最佳奖励等）

---

## 五、收敛检测说明

### 5.1 收敛判断标准
当最近 `convergence_window`（默认100）个episode的奖励标准差小于平均奖励的 `convergence_threshold`（默认1%）时，标记为收敛。

### 5.2 收敛检测特性
- **仅监控，不停止训练**: 收敛检测只用于监控和日志记录，不会提前停止训练
- **训练仍按原逻辑进行**: 训练会在达到 `max_episodes` 或时间限制时停止
- **日志记录**: 检测到收敛时会在日志中记录，便于后续分析

---

## 六、稳定性保障

### 6.1 使用tmux会话
训练将在tmux会话中运行，确保：
- SSH断开连接后训练继续运行
- 服务器重启后可以通过tmux重新连接查看进度

### 6.2 检查点保存
- 每50个episode自动保存检查点
- 如果训练中断，可以从检查点恢复（需要手动实现恢复逻辑）

### 6.3 时间限制保护
- 设置88小时训练时间限制
- 达到时间限制后自动停止，不会超时运行

### 6.4 异常处理
- 每个地图训练独立，单个地图失败不影响其他地图
- 所有异常都会记录到日志文件中

---

## 七、运行方法

### 7.1 在tmux会话中运行

```bash
# 1. 进入项目目录
cd /root/zhongjie_test_1/continuous_CBS

# 2. 在tmux会话中运行训练脚本
tmux new -s training_all_maps
# 或者使用现有会话
tmux attach -t training_all_maps

# 3. 运行训练脚本（日志会同时输出到终端和文件）
python train_all_maps.py 2>&1 | tee training_console.log

# 4. 分离会话（让训练在后台运行）
# 按 Ctrl+B 然后按 D
```

### 7.2 查看训练进度

```bash
# 查看tmux会话
tmux ls

# 重新连接到会话
tmux attach -t training_all_maps

# 查看日志文件
tail -f training_log_*.txt
tail -f training_console.log
```

### 7.3 检查训练状态

```bash
# 查看最新的训练日志
ls -lt training_log_*.txt | head -1 | awk '{print $NF}' | xargs tail -50

# 查看已保存的模型
ls -lh ppo_*.zip

# 查看奖励记录
ls -lh rewards_*.csv
```

---

## 八、修改对比

### 8.1 原始 RewardCallback（修改前）

```python
class RewardCallback(BaseCallback):
    def __init__(self, max_episodes, *args, **kwargs):
        super(RewardCallback, self).__init__(*args, **kwargs)
        self.rewards = []
        self.max_episodes = max_episodes
        self.episode_count = 0

    def _on_step(self) -> bool:
        if self.locals['dones'].any():
            self.rewards.append(self.locals['rewards'])
            self.episode_count += 1
            print(f"已完成 {self.episode_count} 个回合")
        if self.episode_count >= self.max_episodes:
            print(f"已完成 {self.max_episodes} 个回合，停止训练")
            return False
        return True
```

### 8.2 增强的 RewardCallback（修改后）

主要增强：
1. 添加收敛检测（convergence_window, convergence_threshold参数）
2. 添加最佳奖励跟踪（best_reward）
3. 添加定期进度日志（每10个episode）
4. 添加收敛状态标记（converged）

核心逻辑保持不变：
- 返回值逻辑不变（达到max_episodes返回False，否则返回True）
- 训练停止条件不变
- 向后兼容（新参数都有默认值）

---

## 九、预期训练时间

根据配置估算：
- **总训练时间限制**: 88小时
- **每个地图的训练任务数**: 24-30个
- **每个环境的训练步数**: 10,000步
- **每个环境的最大回合数**: 1,000回合

**实际训练时间取决于**:
- 每个环境的求解复杂度
- 计算机性能
- 是否有GPU加速

---

## 十、注意事项

1. **磁盘空间**: 确保有足够的磁盘空间存储模型文件和日志（预计需要几GB）
2. **内存**: 训练过程会占用一定内存，确保服务器有足够内存
3. **训练中断**: 如果训练中断，可以从检查点文件恢复（需要手动实现）
4. **日志文件**: 日志文件可能很大（几十MB到几GB），注意磁盘空间
5. **时间限制**: 88小时后训练会自动停止，即使未完成所有地图

---

## 十一、验证清单

开始训练前，请确认：

- [x] 所有地图文件存在（instances/*/map.xml）
- [x] 所有训练任务目录存在（instances/*/train/）
- [x] 依赖包已安装（stable-baselines3, numpy, pandas等）
- [x] 有足够的磁盘空间
- [x] tmux会话已创建
- [x] 代码修改已完成且测试通过
- [x] 超参数配置已确认

---

## 十二、文件清单

本次训练相关的文件：

**修改的文件**:
- `continuous_CBS/ccbsenv.py` - RewardCallback类增强

**新建的文件**:
- `continuous_CBS/train_all_maps.py` - 多地图训练脚本
- `continuous_CBS/训练准备说明.md` - 本说明文档

**运行时生成的文件**:
- `training_log_*.txt` - 训练日志
- `ppo_<地图名>.zip` - 训练好的模型
- `ppo_<地图名>_checkpoint_ep*.zip` - 检查点文件
- `rewards_<地图名>.csv` - 奖励记录

---

**准备完成时间**: 2026-01-04  
**准备人员**: AI Assistant  
**状态**: ✅ 已准备好开始训练

