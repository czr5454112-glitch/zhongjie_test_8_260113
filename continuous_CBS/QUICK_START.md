# å¿«é€Ÿå¼€å§‹è®­ç»ƒæŒ‡å—

## ğŸ“‹ å‡†å¤‡å·¥ä½œæ£€æŸ¥

âœ… ä¾èµ–å·²å®‰è£…ï¼ˆstable_baselines3, gymnasiumï¼‰
âœ… è®­ç»ƒè„šæœ¬å·²å°±ç»ªï¼ˆtrain_ppo_parallel.pyï¼‰
âœ… å®ä¾‹æ•°æ®å·²å°±ç»ª

## ğŸš€ å¼€å§‹è®­ç»ƒï¼ˆ3æ­¥æ“ä½œï¼‰

### æ­¥éª¤1: è¿›å…¥tmuxå¹¶åˆ›å»ºæ—¥å¿—ç›®å½•

```bash
# å¦‚æœè¿˜æ²¡æœ‰tmuxä¼šè¯ï¼Œåˆ›å»ºä¸€ä¸ª
tmux new -s training

# æˆ–è€…è¿›å…¥å·²æœ‰ä¼šè¯
tmux attach -t training

# è¿›å…¥é¡¹ç›®ç›®å½•
cd /root/zhongjie_test_1/continuous_CBS

# åˆ›å»ºæ—¥å¿—ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
mkdir -p logs
```

### æ­¥éª¤2: å¯åŠ¨è®­ç»ƒï¼ˆPane 1ï¼‰

åœ¨tmuxä¸­ï¼Œç›´æ¥è¿è¡Œï¼š

```bash
python train_ppo_parallel.py 2>&1 | tee -a logs/train_stdout.log
```

**è¯´æ˜ï¼š**
- `2>&1` å°†é”™è¯¯è¾“å‡ºä¹Ÿé‡å®šå‘
- `tee -a` åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
- æ—¥å¿—ä¿å­˜åœ¨ `logs/train_stdout.log`

**è®­ç»ƒå‚æ•°ï¼ˆå·²åœ¨ä»£ç ä¸­é…ç½®ï¼‰ï¼š**
- å¹¶è¡Œç¯å¢ƒæ•°ï¼š20ä¸ªï¼ˆæ¯ç§åœ°å›¾4ä¸ªï¼‰
- æœ€å¤§è®­ç»ƒå›åˆï¼š80000
- æ—¶é—´é™åˆ¶ï¼š88å°æ—¶
- æ£€æŸ¥ç‚¹ä¿å­˜ï¼šæ¯200000æ­¥

### æ­¥éª¤3: è®¾ç½®ç›‘æ§ï¼ˆå¯é€‰ä½†æ¨èï¼‰

#### æ–¹å¼A: TensorBoardå¯è§†åŒ–ï¼ˆæ¨èï¼‰

**åœ¨tmuxä¸­åˆ›å»ºæ–°paneï¼š**
```
æŒ‰ Ctrl+bï¼Œç„¶åæŒ‰ " (æ¨ªå‘åˆ†å±) æˆ– % (ç«–å‘åˆ†å±)
```

**åœ¨æ–°paneä¸­å¯åŠ¨TensorBoardï¼š**
```bash
cd /root/zhongjie_test_1/continuous_CBS
tensorboard --logdir ./logs/tensorboard --host 127.0.0.1 --port 6006
```

**åœ¨æœ¬åœ°ç”µè„‘å»ºç«‹SSHç«¯å£è½¬å‘ï¼š**
```bash
# åœ¨æœ¬åœ°ç»ˆç«¯æ‰§è¡Œï¼ˆä¿æŒè¿æ¥ï¼‰
ssh -L 6006:127.0.0.1:6006 root@your-server-ip
```

**åœ¨æœ¬åœ°æµè§ˆå™¨æ‰“å¼€ï¼š**
```
http://127.0.0.1:6006
```

#### æ–¹å¼B: ç›´æ¥æŸ¥çœ‹æ—¥å¿—ï¼ˆè½»é‡çº§ï¼‰

**åœ¨tmuxæ–°paneä¸­ï¼š**
```bash
tail -f logs/train_stdout.log
```

#### æ–¹å¼C: ä½¿ç”¨ç›‘æ§è„šæœ¬

**åœ¨tmuxæ–°paneä¸­ï¼š**
```bash
bash monitor_training.sh
```

## ğŸ“Š ç›‘æ§è¦ç‚¹

### åœ¨TensorBoardä¸­å…³æ³¨ï¼š
1. **rollout/ep_rew_mean** - å›æŠ¥å‡å€¼ï¼ˆåº”è¯¥ä¸Šå‡å¹¶ç¨³å®šï¼‰
2. **rollout/ep_len_mean** - å›åˆé•¿åº¦ï¼ˆåº”è¯¥ä¸‹é™ï¼‰
3. **train/approx_kl** - KLæ•£åº¦ï¼ˆåº”è¯¥ç¨³å®šåœ¨0.01-0.1ï¼‰
4. **train/entropy_loss** - ç†µæŸå¤±ï¼ˆåº”è¯¥é€æ­¥ä¸‹é™ï¼‰

### åœ¨æ—¥å¿—ä¸­å…³æ³¨ï¼š
- æ¯10ä¸ªepisodeä¼šæ‰“å°ç»Ÿè®¡ä¿¡æ¯
- çœ‹åˆ° `[æ”¶æ•›æ£€æµ‹]` æç¤ºè¯´æ˜å¯èƒ½å·²æ”¶æ•›
- çœ‹åˆ° `[æ£€æŸ¥ç‚¹]` è¯´æ˜æ¨¡å‹å·²ä¿å­˜

### ç¤ºä¾‹è¾“å‡ºï¼š
```
Episode 10/80000: æœ€è¿‘å¹³å‡å¥–åŠ±=-2.3456, æœ€ä½³å¥–åŠ±=5.1234, æ”¶æ•›çŠ¶æ€=è®­ç»ƒä¸­
Episode 20/80000: æœ€è¿‘å¹³å‡å¥–åŠ±=-1.2345, æœ€ä½³å¥–åŠ±=5.1234, æ”¶æ•›çŠ¶æ€=è®­ç»ƒä¸­
...
[æ”¶æ•›æ£€æµ‹] Episode 500: æ£€æµ‹åˆ°æ”¶æ•›! æœ€è¿‘100ä¸ªepisodeçš„å¹³å‡å¥–åŠ±=6.1234, æ ‡å‡†å·®=0.0123
[æ£€æŸ¥ç‚¹] ä¿å­˜æ£€æŸ¥ç‚¹åˆ°: ppo_ccbs_multi_map_checkpoint_step200000 (æ­¥æ•°: 200000)
```

## ğŸ›‘ åœæ­¢è®­ç»ƒ

å¦‚æœéœ€è¦åœæ­¢è®­ç»ƒï¼š
```bash
# åœ¨è®­ç»ƒpaneä¸­æŒ‰ Ctrl+C
# æˆ–è€…
tmux kill-session -t training
```

è®­ç»ƒä¼šè‡ªåŠ¨ä¿å­˜æ£€æŸ¥ç‚¹ï¼Œå¯ä»¥éšæ—¶ä¸­æ–­å’Œæ¢å¤ã€‚

## ğŸ“ è¾“å‡ºæ–‡ä»¶ä½ç½®

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šç”Ÿæˆï¼š
- `logs/train_stdout.log` - å®Œæ•´è®­ç»ƒæ—¥å¿—
- `logs/monitor.csv` - ç¯å¢ƒç›‘æ§æ•°æ®
- `logs/tensorboard/` - TensorBoardæ—¥å¿—
- `checkpoints/ppo_ccbs_policy_*` - æ£€æŸ¥ç‚¹æ¨¡å‹
- `ppo_ccbs_multi_map_final.zip` - æœ€ç»ˆæ¨¡å‹ï¼ˆè®­ç»ƒå®Œæˆåï¼‰
- `rewards_ppo_multi_map.csv` - å¥–åŠ±è®°å½•

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **è®­ç»ƒæ—¶é—´**ï¼šé¢„è®¡éœ€è¦æ•°å°æ—¶åˆ°æ•°åå°æ—¶ï¼Œå–å†³äºç¡¬ä»¶æ€§èƒ½
2. **GPUä½¿ç”¨**ï¼šå¦‚æœæœ‰GPUï¼Œä¼šè‡ªåŠ¨ä½¿ç”¨
3. **å†…å­˜å ç”¨**ï¼š20ä¸ªå¹¶è¡Œç¯å¢ƒä¼šå ç”¨è¾ƒå¤šå†…å­˜
4. **ç£ç›˜ç©ºé—´**ï¼šç¡®ä¿æœ‰è¶³å¤Ÿç©ºé—´ä¿å­˜æ£€æŸ¥ç‚¹å’Œæ—¥å¿—

## ğŸ”§ å¸¸è§é—®é¢˜

### å¦‚æœè®­ç»ƒå¡ä½æˆ–æŠ¥é”™ï¼š
```bash
# æŸ¥çœ‹è¯¦ç»†é”™è¯¯
tail -n 100 logs/train_stdout.log

# æ£€æŸ¥GPU/CPUä½¿ç”¨æƒ…å†µ
nvidia-smi  # å¦‚æœæœ‰GPU
htop  # æŸ¥çœ‹CPUå’Œå†…å­˜
```

### å¦‚æœTensorBoardçœ‹ä¸åˆ°æ•°æ®ï¼š
```bash
# æ£€æŸ¥æ—¥å¿—ç›®å½•
ls -lh logs/tensorboard/
find logs/tensorboard -type f | head -10
```

### å¦‚æœéœ€è¦è°ƒæ•´å‚æ•°ï¼š
ç¼–è¾‘ `train_ppo_parallel.py` é¡¶éƒ¨çš„é…ç½®åŒºåŸŸï¼š
- `PPO_CONFIG` - PPOè¶…å‚æ•°
- `PARALLEL_CONFIG` - å¹¶è¡Œç¯å¢ƒé…ç½®
- `TRAINING_TIME_LIMIT_HOURS` - æ—¶é—´é™åˆ¶

## âœ… å¼€å§‹è®­ç»ƒ

ç°åœ¨ä½ å¯ä»¥æ‰§è¡Œï¼š

```bash
cd /root/zhongjie_test_1/continuous_CBS
python train_ppo_parallel.py 2>&1 | tee -a logs/train_stdout.log
```

ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸ‰


