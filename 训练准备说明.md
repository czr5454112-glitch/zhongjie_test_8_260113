# 训练准备说明文档

**创建时间**: 2026-01-12  
**训练目标**: 对所有地图类型进行PPO通用模型训练（100小时训练时间限制，200万步）

---

## 一、代码修改说明

### 1.1 修改的文件：`ccbsenv.py`

**修改位置**: `RewardCallback` 类（第266-316行）

**修改内容**:
- 添加收敛检测功能
- 添加训练进度监控和日志输出
- 添加最佳奖励跟踪

**关键修改点**:
```python
# 新增参数（都有默认值，向后兼容）:
- convergence_window=100        # 收敛检测窗口大小
- convergence_threshold=0.01    # 收敛阈值（奖励变化率）
- best_reward                   # 跟踪最佳奖励
- converged                     # 收敛状态标志

# 新增功能:
1. 收敛检测：当最近N个episode的奖励标准差小于阈值时，标记为收敛
2. 定期日志：每10个episode打印训练进度和收敛状态
3. 最佳奖励跟踪：记录训练过程中的最佳奖励值
```

**向后兼容性**: ✅ 完全兼容，原有代码无需修改

**核心逻辑不变**: 
- 返回值逻辑不变（达到max_episodes返回False，否则返回True）
- 训练停止条件不变
- PPO训练流程不变

---

### 1.2 训练脚本：`train_ppo_parallel.py`

**文件说明**: 通用模型并行训练脚本，使用SubprocVecEnv实现多地图并行训练

**主要功能**:
1. 支持多地图并行训练（empty-16-16-random, room-64-64-8_random, den520d_random, warehouse-10-20-random, roadmap-sparse）
2. 时间限制控制（100小时）
3. 总训练步数：200万步
4. 检查点保存（每20万步）
5. TensorBoard日志记录
6. 详细日志记录（包含收敛状态）
7. 自动保存训练结果和奖励记录

**训练方式**: 使用并行环境训练一个通用模型，适用于所有地图类型

---

## 二、超参数配置

### 2.1 PPO训练配置

```python
PPO_CONFIG = {
    "learning_rate": 3e-4,              # 学习率
    "gamma": 0.999,                     # 折扣因子（提高到0.999）
    "gae_lambda": 0.95,                 # GAE lambda参数
    "clip_range": 0.2,                  # PPO裁剪范围
    "ent_coef": 0.01,                   # 熵系数（鼓励探索）
    "n_steps": 256,                     # 每个环境采样步数
    "batch_size": 1024,                 # 批大小
    "n_epochs": 10,                     # 每批优化轮数
    "max_episodes": 80000,              # 最大训练回合数
    "total_timesteps": 2000000,         # 总训练步数（200万步）
    "checkpoint_freq": 200000,          # 检查点保存频率（每20万步）
}
```

### 2.2 CCBS算法配置

```python
CCBS_CONFIG = {
    "agent_size": 0.5,                      # 智能体尺寸 (0, 0.5]
    "hlh_type": 2,                          # 0-无hlh, 1-使用simplex求解lpp, 2-贪婪选择不相交冲突
    "precision": 0.1,                       # 等待时间确定精度
    "timelimit": 300,                       # 时间限制（秒）
    "use_precalculated_heuristic": False,   # True: 使用反向Dijkstra, False: 使用欧几里得距离
    "use_disjoint_splitting": True,         # 使用不相交分割
    "use_cardinal": True,                   # 优先处理cardinal冲突
    "use_corridor_symmetry": False,         # 使用走廊对称性
    "use_target_symmetry": False,           # 使用目标对称性
    "use_rl": False,                        # 训练时不需要RL模型
    "verbose": True,                        # 是否显示详细信息
}
```

### 2.3 强化学习环境配置

```python
RL_ENV_CONFIG = {
    "max_step": 1024,                  # 最大搜索步数
    "reward_1": 10,                    # 当前节点满足约束且无其他冲突的奖励
    "reward_2": 2,                     # 分支数量权重系数
    "reward_3": -5,                    # 当前节点不满足约束的惩罚
    "reward_iter_pos": -0.5,           # 迭代位置奖励
    "cost_weight": 0.01,               # 目标函数权重
    "cardinal_conflicts_weight": 1,    # cardinal冲突权重
    "semicard_conflicts_weight": 1,    # semi-cardinal冲突权重
    "non_cardinal_conflict_weight": 1, # non-cardinal冲突权重
    "max_process_agent": 100,          # 最大处理的智能体数
}
```

### 2.4 训练时间限制

```python
TRAINING_TIME_LIMIT_HOURS = 100  # 100小时训练时间限制
```

---

## 三、训练地图列表

本次训练将使用以下地图类型进行**通用模型**训练（所有地图共享一个模型）：

| 地图名称 | 地图路径 | 训练任务数 | 并行环境数 |
|---------|---------|-----------|-----------|
| empty-16-16-random | instances/empty-16-16-random/map.xml | 24 | 4 |
| room-64-64-8_random | instances/room-64-64-8_random/map.xml | 30 | 4 |
| den520d_random | instances/den520d_random/map.xml | 25 | 4 |
| warehouse-10-20-random | instances/warehouse-10-20-random/map.xml | 25 | 4 |
| roadmap-sparse | instances/roadmaps/sparse/map.xml | 25 | 4 |

**训练方式**: 
- 使用并行环境（SubprocVecEnv）同时训练所有地图
- 总共20个并行环境（5种地图 × 4个环境/地图）
- 训练一个通用模型 `ppo_ccbs_multi_map_final.zip`，适用于所有地图类型

---

## 四、输出文件说明

### 4.1 模型文件
训练完成后会生成：
- `ppo_ccbs_multi_map_final.zip` - 训练好的通用模型文件（stable-baselines3格式）
- `checkpoints/ppo_ccbs_policy_<步数>_steps.zip` - 训练过程中的检查点文件（每20万步保存一次）

### 4.2 日志文件
- `logs/train_stdout.log` - 训练标准输出日志
- `logs/tensorboard/` - TensorBoard日志目录（包含训练曲线数据）
- `logs/monitor.csv` - 训练监控数据（episode奖励等）
- `rewards_ppo_multi_map.csv` - 奖励记录（包含episode和reward列）

### 4.3 日志内容
训练日志包含以下信息：
- 训练开始时间和时间限制
- 每个地图的训练进度
- 每个环境（训练任务）的训练状态
- 收敛检测结果（收敛状态、平均奖励、最佳奖励）
- 检查点保存记录
- 训练完成后的统计信息（训练时间、总回合数、收敛状态、最佳奖励等）

---

## 五、收敛检测说明

### 5.1 收敛判断标准
当最近 `convergence_window`（默认100）个episode的奖励标准差小于平均奖励的 `convergence_threshold`（默认1%）时，标记为收敛。

### 5.2 收敛检测特性
- **仅监控，不停止训练**: 收敛检测只用于监控和日志记录，不会提前停止训练
- **训练仍按原逻辑进行**: 训练会在达到 `max_episodes` 或时间限制时停止
- **日志记录**: 检测到收敛时会在日志中记录，便于后续分析

---

## 六、稳定性保障

### 6.1 使用tmux会话
训练将在tmux会话中运行，确保：
- SSH断开连接后训练继续运行
- 服务器重启后可以通过tmux重新连接查看进度

### 6.2 检查点保存
- 每50个episode自动保存检查点
- 如果训练中断，可以从检查点恢复（需要手动实现恢复逻辑）

### 6.3 时间限制保护
- 设置88小时训练时间限制
- 达到时间限制后自动停止，不会超时运行

### 6.4 异常处理
- 每个地图训练独立，单个地图失败不影响其他地图
- 所有异常都会记录到日志文件中

---

## 七、运行方法

### 7.1 在tmux会话中运行

```bash
# 1. 进入项目目录
cd /root/zhongjie_test_1/continuous_CBS

# 2. 在tmux会话中运行训练脚本
tmux new -s training_parallel
# 或者使用现有会话
tmux attach -t training_parallel

# 3. 运行训练脚本（日志会同时输出到终端和文件）
python train_ppo_parallel.py 2>&1 | tee logs/train_stdout.log

# 4. 分离会话（让训练在后台运行）
# 按 Ctrl+B 然后按 D
```

### 7.2 查看训练进度

```bash
# 查看tmux会话
tmux ls

# 重新连接到会话
tmux attach -t training_parallel

# 查看日志文件
tail -f logs/train_stdout.log

# 查看TensorBoard（在另一个终端）
tensorboard --logdir logs/tensorboard --port 6006
```

### 7.3 检查训练状态

```bash
# 查看最新的训练日志
tail -50 logs/train_stdout.log

# 查看已保存的模型
ls -lh ppo_ccbs_multi_map*.zip
ls -lh checkpoints/

# 查看奖励记录
ls -lh rewards_ppo_multi_map.csv

# 查看TensorBoard日志
ls -lh logs/tensorboard/
```

---

## 八、修改对比

### 8.1 原始 RewardCallback（修改前）

```python
class RewardCallback(BaseCallback):
    def __init__(self, max_episodes, *args, **kwargs):
        super(RewardCallback, self).__init__(*args, **kwargs)
        self.rewards = []
        self.max_episodes = max_episodes
        self.episode_count = 0

    def _on_step(self) -> bool:
        if self.locals['dones'].any():
            self.rewards.append(self.locals['rewards'])
            self.episode_count += 1
            print(f"已完成 {self.episode_count} 个回合")
        if self.episode_count >= self.max_episodes:
            print(f"已完成 {self.max_episodes} 个回合，停止训练")
            return False
        return True
```

### 8.2 增强的 RewardCallback（修改后）

主要增强：
1. 添加收敛检测（convergence_window, convergence_threshold参数）
2. 添加最佳奖励跟踪（best_reward）
3. 添加定期进度日志（每10个episode）
4. 添加收敛状态标记（converged）

核心逻辑保持不变：
- 返回值逻辑不变（达到max_episodes返回False，否则返回True）
- 训练停止条件不变
- 向后兼容（新参数都有默认值）

---

## 九、预期训练时间

根据配置估算：
- **总训练时间限制**: 100小时
- **总训练步数**: 200万步（2,000,000步）
- **并行环境数**: 20个（5种地图 × 4个环境/地图）
- **最大训练回合数**: 80,000回合
- **检查点保存频率**: 每20万步

**实际训练时间取决于**:
- 每个环境的求解复杂度
- 计算机性能
- 是否有GPU加速
- 并行环境的效率

---

## 十、注意事项

1. **磁盘空间**: 确保有足够的磁盘空间存储模型文件和日志（预计需要几GB）
2. **内存**: 并行训练会占用更多内存（20个并行环境），确保服务器有足够内存
3. **训练中断**: 如果训练中断，可以从检查点文件恢复（需要手动实现）
4. **日志文件**: 日志文件可能很大（几十MB到几GB），注意磁盘空间
5. **时间限制**: 100小时后训练会自动停止，即使未达到200万步
6. **TensorBoard**: 训练过程中会自动记录TensorBoard日志，可以通过TensorBoard实时查看训练曲线
7. **通用模型**: 本次训练的是通用模型，适用于所有地图类型，而不是为每个地图单独训练

---

## 十一、验证清单

开始训练前，请确认：

- [x] 所有地图文件存在（instances/*/map.xml）
- [x] 所有训练任务目录存在（instances/*/train/）
- [x] 依赖包已安装（stable-baselines3, numpy, pandas等）
- [x] 有足够的磁盘空间
- [x] tmux会话已创建
- [x] 代码修改已完成且测试通过
- [x] 超参数配置已确认

---

## 十二、文件清单

本次训练相关的文件：

**修改的文件**:
- `continuous_CBS/ccbsenv.py` - RewardCallback类增强

**训练脚本**:
- `continuous_CBS/train_ppo_parallel.py` - 通用模型并行训练脚本
- `continuous_CBS/训练准备说明.md` - 本说明文档

**运行时生成的文件**:
- `logs/train_stdout.log` - 训练标准输出日志
- `logs/tensorboard/` - TensorBoard日志目录
- `logs/monitor.csv` - 训练监控数据
- `ppo_ccbs_multi_map_final.zip` - 训练好的通用模型
- `checkpoints/ppo_ccbs_policy_*.zip` - 检查点文件
- `rewards_ppo_multi_map.csv` - 奖励记录

---

**准备完成时间**: 2026-01-12  
**准备人员**: AI Assistant  
**状态**: ✅ 已准备好开始训练

**本次训练配置**:
- 训练时间限制: 100小时
- 总训练步数: 200万步（2,000,000步）
- 训练方式: 并行环境训练通用模型
- 并行环境数: 20个
- 检查点频率: 每20万步

